{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Homework_M6_dilan_nebioglu (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeERWemNFXXt"
      },
      "source": [
        "# Homework: classify the origin of names using a character-level RNN\n",
        "\n",
        "In this homework we will use an rnn-based model to perform classification. The goal is threefold:\n",
        "\n",
        "1. Get more hands on with the preprocessing needed to perform text classification from A to Z. No preprocessing is done for you!\n",
        "2. Use embeddings and RNNs in conjunction at the character level to perform classification.\n",
        "3. Write a function that takes as input a string, and outputs the name of the predicted class.\n",
        "\n",
        "However, here are guidelines to help you through all the steps:\n",
        "\n",
        "1. Figure out the number of classes, and map the classes to integers (or one-hot vectors). This is needed for fitting the model and training it to do classification.\n",
        "2. Use the keras tokenizer at the character level to tokenize your input into integer sequences.\n",
        "3. Pad your sequences using the keras preprocessing tools.\n",
        "4. Build a model that uses, minimally, an embedding layer, an RNN (of your choice) and a dense layer to output the logits or probabilities for the target classes (name origins).\n",
        "5. Fit the model and evaluate on the test set.\n",
        "6. Write a function that takes a string as input and predicts the origin (as its original string value)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld65JPlPLh7L"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import string\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frT0-1E9LjbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfdda26-eb83-436e-e365-4a08c239ece0"
      },
      "source": [
        "# Download the data\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-10 22:02:10--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.32.204.49, 13.32.204.34, 13.32.204.65, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.32.204.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   2.75M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-11-10 22:02:11 (80.9 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BupAEBBLpHx"
      },
      "source": [
        "def read_files():\n",
        "  data = []\n",
        "  unique_origins = []\n",
        "  for filename in glob('data/names/*.txt'):\n",
        "    origin = filename.split('/')[-1].split('.txt')[0]\n",
        "    unique_origins.append(origin)\n",
        "    names = open(filename).readlines()\n",
        "    for name in names:\n",
        "      data.append((name.strip(), origin))\n",
        "  return data, unique_origins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOSLHc4LmTd"
      },
      "source": [
        "def unicode_to_ascii(str):\n",
        "  all_letters = string.ascii_letters + \" .,;'\"\n",
        "  n_letters = len(all_letters)\n",
        "  return ''.join(\n",
        "    c for c in unicodedata.normalize('NFD', str)\n",
        "    if unicodedata.category(c) != 'Mn'\n",
        "    and c in all_letters\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nZvq7bGLrYH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "70e100d0-a336-4f8d-cad2-d54d0405f663"
      },
      "source": [
        "# Create dataset\n",
        "data, unique_origins = read_files()\n",
        "print('unique_origins:', unique_origins)\n",
        "print('len(unique_origins):', len(unique_origins))\n",
        "df = pd.DataFrame(data=data)\n",
        "df.rename({0: 'Name', 1: 'Origin'}, axis=1, inplace=True)\n",
        "\n",
        "# Create categories for y\n",
        "df['Origin'] = df['Origin'].astype('category')\n",
        "df['origin_cat'] = df['Origin'].cat.codes\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unique_origins: ['Korean', 'Greek', 'Chinese', 'Polish', 'French', 'Arabic', 'Scottish', 'Spanish', 'Portuguese', 'Vietnamese', 'German', 'Japanese', 'Italian', 'English', 'Russian', 'Irish', 'Czech', 'Dutch']\n",
            "len(unique_origins): 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Origin</th>\n",
              "      <th>origin_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ahn</td>\n",
              "      <td>Korean</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Baik</td>\n",
              "      <td>Korean</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bang</td>\n",
              "      <td>Korean</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Byon</td>\n",
              "      <td>Korean</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cha</td>\n",
              "      <td>Korean</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Name  Origin  origin_cat\n",
              "0   Ahn  Korean          11\n",
              "1  Baik  Korean          11\n",
              "2  Bang  Korean          11\n",
              "3  Byon  Korean          11\n",
              "4   Cha  Korean          11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7ksDA7TL2di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ce0449-1cd6-45c4-a5fb-610c80df2478"
      },
      "source": [
        "# Create category dictionary\n",
        "y_dictionary = dict(enumerate(df['Origin'].cat.categories))\n",
        "y_dictionary\n",
        "# TODO try to_categorical once more"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Arabic',\n",
              " 1: 'Chinese',\n",
              " 2: 'Czech',\n",
              " 3: 'Dutch',\n",
              " 4: 'English',\n",
              " 5: 'French',\n",
              " 6: 'German',\n",
              " 7: 'Greek',\n",
              " 8: 'Irish',\n",
              " 9: 'Italian',\n",
              " 10: 'Japanese',\n",
              " 11: 'Korean',\n",
              " 12: 'Polish',\n",
              " 13: 'Portuguese',\n",
              " 14: 'Russian',\n",
              " 15: 'Scottish',\n",
              " 16: 'Spanish',\n",
              " 17: 'Vietnamese'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wADd5A_9L7Rw"
      },
      "source": [
        "# Split data into train/test\n",
        "# names, origins = zip(*data)\n",
        "# names_train, names_test, origins_train, origins_test = train_test_split(names, origins, test_size=0.25, shuffle=True, random_state=123)\n",
        "names_train, names_test, origins_train, origins_test = train_test_split(df['Name'], df['origin_cat'], test_size=0.25, shuffle=True, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXqW2qG-L_-Y"
      },
      "source": [
        "# Initialize the encoder/tokenizer and fit it to the text on a character-level\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(df['Name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6TdLTifMBaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ceee4dd-fed0-4db6-f54d-bc8a72e06648"
      },
      "source": [
        "dataset_size = tokenizer.document_count\n",
        "print('dataset_size:', dataset_size)\n",
        "category_size = len(tokenizer.word_index)\n",
        "print('category_size:', category_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset_size: 20074\n",
            "category_size: 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg8zDLpfMC_U"
      },
      "source": [
        "# Prepare X\n",
        "def prepare_x(x):\n",
        "  names = x\n",
        "  unicoded_names = names.apply(lambda name: unicode_to_ascii(name))\n",
        "  tokenized_names = [tokenizer.texts_to_sequences([name]) for name in unicoded_names]\n",
        "  flattened_names = [y for x in tokenized_names for y in x]\n",
        "  padded_names = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    flattened_names, padding=\"post\"\n",
        "  )\n",
        "  one_hot_encoded_names = tf.one_hot(padded_names, depth=category_size)\n",
        "  return one_hot_encoded_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ctSV5Y9MGjN"
      },
      "source": [
        "X_train = prepare_x(names_train)\n",
        "X_test = prepare_x(names_test)\n",
        "y_train = origins_train\n",
        "y_test = origins_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_6mav_YMOMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14def9d-ed6f-4eae-bb98-c9a1ca5ccc5a"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8460      4\n",
              "19082     8\n",
              "14631    14\n",
              "7387      4\n",
              "9252      4\n",
              "         ..\n",
              "7763      4\n",
              "15377    14\n",
              "17730    14\n",
              "15725    14\n",
              "19966     3\n",
              "Name: origin_cat, Length: 15055, dtype: int8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1D_X-XjMP8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf3b5a6-5323-4375-f9e5-928bee796d16"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(15055, 19, 58), dtype=float32, numpy=\n",
              "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucUz6v26MLSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8afe2b60-84d5-4840-a8a7-126e4b070646"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  # tf.keras.layers.Embedding(input_dim=(category_size+1),\n",
        "  #                           output_dim=64,\n",
        "  #                           mask_zero=True),\n",
        "  tf.keras.layers.LSTM(128, return_sequences= True, input_shape=[None, category_size]),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.LSTM(128),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(category_size, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, None, 128)         95744     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 58)                7482      \n",
            "=================================================================\n",
            "Total params: 234,810\n",
            "Trainable params: 234,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZd0gK38MS7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5e1390-c258-4f09-f50b-4c1c1da3bbf6"
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "377/377 [==============================] - 3s 8ms/step - loss: 1.8506 - accuracy: 0.4817 - val_loss: 1.5026 - val_accuracy: 0.5623\n",
            "Epoch 2/20\n",
            "377/377 [==============================] - 2s 7ms/step - loss: 1.4756 - accuracy: 0.5771 - val_loss: 1.3125 - val_accuracy: 0.6274\n",
            "Epoch 3/20\n",
            "377/377 [==============================] - 2s 7ms/step - loss: 1.3357 - accuracy: 0.6217 - val_loss: 1.2258 - val_accuracy: 0.6606\n",
            "Epoch 4/20\n",
            "377/377 [==============================] - 2s 7ms/step - loss: 1.2494 - accuracy: 0.6475 - val_loss: 1.1803 - val_accuracy: 0.6652\n",
            "Epoch 5/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 1.1774 - accuracy: 0.6668 - val_loss: 1.0951 - val_accuracy: 0.6862\n",
            "Epoch 6/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 1.1072 - accuracy: 0.6824 - val_loss: 1.0579 - val_accuracy: 0.6911\n",
            "Epoch 7/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 1.0612 - accuracy: 0.6924 - val_loss: 1.0660 - val_accuracy: 0.6855\n",
            "Epoch 8/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 1.0019 - accuracy: 0.7059 - val_loss: 0.9547 - val_accuracy: 0.7207\n",
            "Epoch 9/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.9218 - accuracy: 0.7292 - val_loss: 0.8997 - val_accuracy: 0.7356\n",
            "Epoch 10/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.8625 - accuracy: 0.7416 - val_loss: 0.8521 - val_accuracy: 0.7486\n",
            "Epoch 11/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.8153 - accuracy: 0.7531 - val_loss: 0.8045 - val_accuracy: 0.7612\n",
            "Epoch 12/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.7594 - accuracy: 0.7687 - val_loss: 0.7618 - val_accuracy: 0.7718\n",
            "Epoch 13/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.7253 - accuracy: 0.7794 - val_loss: 0.7358 - val_accuracy: 0.7742\n",
            "Epoch 14/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.6885 - accuracy: 0.7890 - val_loss: 0.7953 - val_accuracy: 0.7629\n",
            "Epoch 15/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.6586 - accuracy: 0.7965 - val_loss: 0.7157 - val_accuracy: 0.7801\n",
            "Epoch 16/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.6355 - accuracy: 0.8024 - val_loss: 0.6992 - val_accuracy: 0.7884\n",
            "Epoch 17/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.6089 - accuracy: 0.8078 - val_loss: 0.6734 - val_accuracy: 0.7944\n",
            "Epoch 18/20\n",
            "377/377 [==============================] - 2s 6ms/step - loss: 0.5865 - accuracy: 0.8126 - val_loss: 0.6662 - val_accuracy: 0.7997\n",
            "Epoch 19/20\n",
            "377/377 [==============================] - 2s 7ms/step - loss: 0.5704 - accuracy: 0.8191 - val_loss: 0.6859 - val_accuracy: 0.7954\n",
            "Epoch 20/20\n",
            "377/377 [==============================] - 2s 7ms/step - loss: 0.5512 - accuracy: 0.8216 - val_loss: 0.6718 - val_accuracy: 0.7918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxeALo2gMU2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bc1393-7747-4155-a8d2-c332f64ea9a8"
      },
      "source": [
        "predictions = np.argmax(model.predict(X_test), axis=-1)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14,  4,  4, ..., 14,  8,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFCoDv2tMY-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c5300c-37ea-4759-d6d2-49fe8af8d5ac"
      },
      "source": [
        "def convert_from_category_to_origin(y_category):\n",
        "  return y_dictionary[y_category]\n",
        "\n",
        "origins_test_original = [convert_from_category_to_origin(category) for category in origins_test]\n",
        "prediction_origin = [convert_from_category_to_origin(category) for category in predictions]\n",
        "\n",
        "print('origins_test_original[:5]', origins_test_original[:5])\n",
        "print('prediction_origin[:5]', prediction_origin[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "origins_test_original[:5] ['Russian', 'German', 'Dutch', 'Czech', 'English']\n",
            "prediction_origin[:5] ['Russian', 'English', 'English', 'Russian', 'English']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpsLaVujMdHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b521f6c-ae21-4e4c-ff73-835764417130"
      },
      "source": [
        "results = model.evaluate(X_test, y_test)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 1s 4ms/step - loss: 0.6628 - accuracy: 0.7980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6627506017684937, 0.7979677319526672]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}